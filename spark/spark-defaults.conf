# General configuration
spark.hadoop.fs.defaultFS=hdfs://hdfs-namenode:9000
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=hdfs://hdfs-namenode:9000/default
spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Default catalog configuration
spark.sql.catalog.spark_catalog=org.apache.spark.sql.catalog.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive
spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083
spark.sql.catalog.spark_catalog.warehouse=hdfs://hdfs-namenode:9000/default

# Iceberg configuration
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
spark.sql.catalog.iceberg.warehouse=hdfs://hdfs-namenode:9000/iceberg

# Dynamic Partitioning for Hive
spark.hadoop.hive.exec.dynamic.partition=true
spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict

# Executor and Driver Resources
spark.deploy.defaultCores=1
spark.executor.instances=2
spark.executor.memory=100m
spark.executor.cores=2
spark.driver.memory=100m
spark.driver.cores=2
spark.worker.memory=400m
spark.worker.cores=1

# Dynamic Allocation
# spark.dynamicAllocation.enabled=true
# spark.dynamicAllocation.minExecutors=1
# spark.dynamicAllocation.maxExecutors=2
# spark.dynamicAllocation.initialExecutors=1

# Shuffle
spark.shuffle.service.enabled=true
#spark.sql.shuffle.partitions=200

# Skew handling
#spark.sql.adaptive.enabled=true

# Scheduler
spark.scheduler.mode=FAIR

# Spark Master URL
spark.master=spark://spark-master:7077