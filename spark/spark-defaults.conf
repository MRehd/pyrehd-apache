# General configuration
spark.hadoop.fs.defaultFS=hdfs://hdfs-namenode:9000
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=hdfs://hdfs-namenode:9000/default
spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Default catalog configuration
spark.sql.catalog.spark_catalog=org.apache.spark.sql.catalog.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive
spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083
spark.sql.catalog.spark_catalog.warehouse=hdfs://hdfs-namenode:9000/default

# Iceberg configuration
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
spark.sql.catalog.iceberg.warehouse=hdfs://hdfs-namenode:9000/iceberg

# Dynamic Partitioning for Hive
spark.hadoop.hive.exec.dynamic.partition=true
spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict

# Executor and Driver Resources
spark.executor.memory=500m
spark.executor.cores=1
spark.driver.memory=500m
spark.driver.cores=1

# Dynamic Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=1
spark.dynamicAllocation.initialExecutors=1
spark.shuffle.service.enabled=true

# Spark Master URL
spark.master=spark://spark-master:7077